---
permalink: /
title: "Exploring the Inner Workings of Large Language Models"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<div class="announcement-box">
<p>üéì <strong>Open to Opportunities:</strong> I am currently seeking PhD positions and research internships in the field of Machine Learning and AI, particularly focusing on LLM interpretability and mechanistic understanding.</p>
</div>

# üîç Current Research: Mechanistic Interpretability in LLMs

As a master's thesis student at [Bethge Lab](https://bethgelab.org/), working under the guidance of [Prof. Matthias Bethge](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/physik/institute/institut-fuer-theoretische-physik/arbeitsgruppen/ag-bethge/) and [Dr. Cagatay Yildiz](https://cagatayyildiz.github.io/), I'm delving deep into the fascinating world of large language models. My research focuses on developing novel approaches to quantify and track knowledge acquisition in these complex systems through mechanistic interpretability.

## üß™ Current Research Directions

Our research extends [Geva et al.'s (2023)](https://arxiv.org/abs/2304.14767) foundational work on factual associations in auto-regressive models, developing more sophisticated measurement techniques. A critical insight emerged from [√ñncel et al.'s (2024)](https://arxiv.org/abs/2410.05581) findings that traditional perplexity metrics can be deceptive ‚Äì high perplexity scores don't always indicate true domain understanding.

I'm currently advancing research along two primary axes:

1. **Knowledge Measurement Pipeline**: Developing transferable methodologies for measuring domain-specific knowledge across model layers, building on ["Dissecting Recall of Factual Associations in Auto-Regressive Language Models"](https://arxiv.org/abs/2304.14767).

2. **Domain-Specific Pre-training**: Investigating how specialized pre-training shapes model capabilities, extending insights from ["Investigating Continual Pretraining in Large Language Models"](https://arxiv.org/abs/2402.17400).

A key innovation in our approach is the application of activation engineering through steering vectors ‚Äì essentially creating interpretable maps of knowledge representation across model layers. This builds on [Arditi et al.'s (2024)](https://arxiv.org/abs/2406.11717) demonstration that LLMs encode features as linear directions in their activation space, complemented by [Merullo et al.'s (2024)](https://arxiv.org/abs/2305.16130) work on vector arithmetic in language models.

## üí° Why Mechanistic Interpretability?

Richard Feynman's principle, "What I cannot create, I do not understand," perfectly encapsulates the mission of mechanistic interpretability. The field has achieved remarkable breakthroughs in demystifying neural networks, from [Elhage et al.'s (2021)](https://transformer-circuits.pub/2021/framework/index.html) mathematical frameworks for transformer circuits to [Elhage et al.'s (2022)](https://transformer-circuits.pub/2022/toy_model/index.html) understanding of superposition phenomena. Recent advances in automated circuit discovery [(Conmy et al., 2023)](https://arxiv.org/abs/2304.14997) and interpretable circuits in GPT-2 [(Chan et al., 2023)](https://arxiv.org/abs/2211.00593) demonstrate that systematic reverse engineering of these complex systems is not just possible ‚Äì it's revolutionizing our understanding.

## üî¨ Practical Applications

These theoretical advances have enabled significant practical breakthroughs:

- Enhanced model efficiency through targeted knowledge editing [(Meng et al., 2023)](link) [(Dai et al., 2022)](link)
- Direct model control via activation engineering [(Turner et al., 2024)](link) [(Geiger et al., 2023)](link) [(Dabral et al., 2023)](link)
- Advanced safety mechanisms through representation engineering [(Arditi et al., 2024)](link) [(Nanda et al., 2023)](link)
- Performance optimization through targeted representation modifications [(Sharma et al., 2023)](link) [(Anthropic et al., 2024)](link)
- More robust evaluation methodologies [(Geva et al., 2023)](link) [(Das et al., 2024)](link)

## üéØ Research Focus

My research explores the crucial intersection of knowledge representation and model behavior, building on work by [Dai et al. (2023)](link) and [Mitchell et al. (2023)](link). I investigate how language models encode and manipulate information, particularly in domain-specific learning contexts [(Zhang et al., 2023)](link). Through activation engineering and steering vector approaches [(Sharma et al., 2023)](link) [(Turner et al., 2024)](link), we're developing precise methods for model control and adaptation while preserving core capabilities.

The most compelling aspect of this work lies in its potential to transform theoretical insights into practical applications, advancing both our understanding and our ability to create more reliable, controllable AI systems.
