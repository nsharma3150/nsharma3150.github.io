---
permalink: /
title: "Exploring the Inner Workings of Large Language Models"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# üîç Current Research: Mechanistic Interpretability in LLMs

As a master's thesis student at [Bethge Lab](https://bethgelab.org/), working under the guidance of [Prof. Matthias Bethge](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/physik/institute/institut-fuer-theoretische-physik/arbeitsgruppen/ag-bethge/) and [Dr. Cagatay Yildiz](https://cagatayyildiz.github.io/), I'm delving deep into the fascinating world of large language models. My research focuses on developing novel approaches to quantify and track knowledge acquisition in these complex systems through mechanistic interpretability.

## üß™ Current Research Directions

Our research extends [Geva et al.'s (2023)](https://arxiv.org/abs/2304.14767) foundational work on factual associations in auto-regressive models, developing more sophisticated measurement techniques. A critical insight emerged from [√ñncel et al.'s (2024)](https://arxiv.org/abs/2410.05581) findings that traditional perplexity metrics can be deceptive.

I'm currently advancing research along two primary axes:

1. **Knowledge Measurement Pipeline**: Developing transferable methodologies for measuring domain-specific knowledge across model layers, building on ["Dissecting Recall of Factual Associations in Auto-Regressive Language Models"](link). 

2. **Domain-Specific Pre-training**: Investigating how specialized pre-training shapes model capabilities, extending insights from ["Investigating Continual Pretraining in Large Language Models"](link).

Our approach innovates through activation engineering with steering vectors, building on [Arditi et al.'s (2024)](link) demonstration of linear feature encoding and [Merullo et al.'s (2024)](link) work on vector arithmetic in language models.

## üí° Why Mechanistic Interpretability?

Drawing inspiration from Richard Feynman's principle, "What I cannot create, I do not understand," the field has achieved remarkable breakthroughs, from [Elhage et al.'s (2021)](link) mathematical frameworks for transformer circuits to [Elhage et al.'s (2022)](link) understanding of superposition phenomena. Recent advances in [automated circuit discovery](link) [(Conmy et al., 2023)](link) and [interpretable circuits in GPT-2](link) [(Chan et al., 2023)](link) demonstrate the potential of systematic reverse engineering.

## üî¨ Practical Applications

These theoretical advances have enabled significant practical breakthroughs:

- Enhanced model efficiency through targeted knowledge editing [(Meng et al., 2023)](link) [(Dai et al., 2022)](link)
- Direct model control via activation engineering [(Turner et al., 2024)](link) [(Geiger et al., 2023)](link) [(Dabral et al., 2023)](link)
- Advanced safety mechanisms [(Arditi et al., 2024)](link) [(Nanda et al., 2023)](link)
- Performance optimization [(Sharma et al., 2023)](link) [(Anthropic et al., 2024)](link)
- Robust evaluation methodologies [(Geva et al., 2023)](link) [(Das et al., 2024)](link)

## üéØ Current Focus

My research explores the crucial intersection of knowledge representation and model behavior, building on work by [Dai et al. (2023)](link) and [Mitchell et al. (2023)](link). I investigate how language models encode information, particularly in domain-specific contexts [(Zhang et al., 2023)](link). Through activation engineering and steering vector approaches [(Sharma et al., 2023)](link) [(Turner et al., 2024)](link), we're developing precise methods for model control while preserving core capabilities.

The most compelling aspect of this work lies in its potential to transform theoretical insights into practical applications, advancing both our understanding and our ability to create more reliable, controllable AI systems.

![Research Overview](main_image.png)
