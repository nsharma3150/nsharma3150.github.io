---
permalink: /
title: "Exploring the Inner Workings of Large Language Models"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# üîç Current Research: Mechanistic Interpretability in LLMs

As a master's thesis student at Bethge Lab, working under the guidance of Prof. Matthias Bethge and Dr. Cagatay Yildiz, I'm delving deep into the fascinating world of large language models. My research focuses on developing novel approaches to quantify and track knowledge acquisition in these complex systems through mechanistic interpretability.

## üß™ Current Research Directions

I'm currently advancing research along two primary axes:

1. **Knowledge Measurement Pipeline**: Developing transferable methodologies for measuring domain-specific knowledge across model layers, building on Geva et al.'s (2023) groundbreaking work on factual associations in auto-regressive models.

2. **Domain-Specific Pre-training**: Investigating how specialized pre-training shapes a model's broader capabilities, with a particular focus on activation engineering through steering vectors.

## üí° Why Mechanistic Interpretability?

Drawing inspiration from Richard Feynman's principle, "What I cannot create, I do not understand," my research aims to demystify neural networks. Recent breakthroughs in the field have enabled:

- Enhanced model efficiency through targeted knowledge editing
- Direct model control via activation engineering
- Advanced safety mechanisms through representation engineering
- Performance optimization through targeted modifications
- Robust evaluation methodologies based on knowledge storage mechanisms

## üî¨ Research Innovation

Our approach introduces novel techniques for creating interpretable maps of knowledge representation across model layers. This builds on recent demonstrations that LLMs encode features as linear directions in their activation space, complemented by advances in vector arithmetic in language models.

## üéØ Impact and Applications

The most compelling aspect of this work lies in its potential to transform theoretical insights into practical applications. Our research aims to advance both our understanding of these complex systems and our ability to create more reliable, controllable AI systems.

![Research Overview](main_image.png)
